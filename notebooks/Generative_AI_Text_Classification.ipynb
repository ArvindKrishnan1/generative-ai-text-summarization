{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1224ff1-aeff-4e29-b202-481832e55ab0",
   "metadata": {},
   "source": [
    "# Generative AI Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e6f12-da61-4f0b-ab64-3526080a77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import s3fs\n",
    "import fs_s3fs\n",
    "import fsspec\n",
    "import json\n",
    "from llama_index.core import TreeIndex, SimpleDirectoryReader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import transformers\n",
    "import mlflow\n",
    "import hyperopt as hp\n",
    "import sphinx\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbad013-ca4e-4666-9764-f040416cb0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download stopwords\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newStopwords = ['b','lt','gt','n','u','ap','reuters'] # Add stopwords \n",
    "\n",
    "for stopword in newStopwords:\n",
    "    stopwords.append(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43282ff1-8419-4736-86e3-cfd2c3a008d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762117e8-eeaf-45ea-98d6-4bbfca6447d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648def37-b389-46cd-a7d8-a0344a83439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Kaggle\n",
    "\n",
    "dataset = \"https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset/data\"\n",
    "od.download(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5b57f-b8de-46d5-a7fd-f5de9f7b5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset, import only 30000 rows of data\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\nickr\\OneDrive\\Documents\\GitHub\\generative-ai-text-summarization\\data\\ag-news-classification-dataset\\ag_news.csv',nrows=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ceb048-4a45-4b4e-ad52-44b5918d6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of dataframe\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925624a-4c1d-4927-808f-5975d06b0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm importation\n",
    "\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb4940-bd0f-403e-9e63-f1d51939090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm shape\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab63fb0-1213-4f55-b10d-3b0c409af02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Description column\n",
    "\n",
    "df = df.drop('Description',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53803b54-f203-406b-9cfc-3931fa943371",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737225c-a1f0-4109-abde-5c2d75506d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find null values and datatypes\n",
    "\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be72a3e-3d19-4901-abdb-7fa1006122fb",
   "metadata": {},
   "source": [
    "There are no null values in the df dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6816ed8-6b16-4f5a-8e8f-86c6745b22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8028175-62cd-4820-a191-1e6ae3652326",
   "metadata": {},
   "source": [
    "There are 1354 duplicate values in the df dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b4a4b-91eb-40f1-8465-add50bb5109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8c488-1f0f-4f07-bf1a-93b1e604194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data set html, special, and non-textual characters\n",
    "\n",
    "def cleaning_text(text):\n",
    "    # Remove HTML tags\n",
    "    cleaning_text = re.sub('<.*?>', '', text)\n",
    "    # Remove special characters and non-textual \n",
    "    cleaning_text = re.sub(r'([^a-zA-Z\\s]|\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', ' ', cleaning_text) # checks plain text for given characters\n",
    "    return cleaning_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479d5c2-b4e2-4c30-9faf-3051b423d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply text cleaning to text in both Description and Title\n",
    "\n",
    "df['Title'] = df['Title'].apply(cleaning_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a1d06-2735-4db7-86ec-acc992362765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the function worked\n",
    "\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518814df-08e0-40e6-8b97-f87cbc98e8e9",
   "metadata": {},
   "source": [
    "Note that in this data set, 1 represents World News, 2 represents Sports News, 3 represents Business News, and 4 represents Sci/Tech news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0223382e-0fd0-4e2a-a2ce-58bf02a2b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to remove stop words\n",
    "\n",
    "stop_words = set(stopwords)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization and lowercasing\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Stop word removal\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e8790-b96c-4391-96c5-1ceb2209ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing to text in Title\n",
    "\n",
    "df['Title'] = df['Title'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85872e09-1b0f-4991-9337-10ffec72a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the function worked\n",
    "\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e870c4d-abfd-4af9-b3e3-200b57aea54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to CSV for ease of use in future\n",
    "\n",
    "cleaned_data_file = r'C:\\Users\\nickr\\OneDrive\\Desktop\\CapstoneTechX\\ag_news_cleaned\\cleaned_ag_news.csv'\n",
    "df.to_csv(cleaned_data_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd8cdb-add6-4426-84db-803400bbc1f3",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d4286e-9003-4783-b52f-b2a3754fbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation data \n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size=.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183cea1-30ce-46b1-9a88-60e5a0dcb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv file for train and test data\n",
    "\n",
    "df_train.to_csv(os.path.join(r'C:\\Users\\nickr\\OneDrive\\Desktop\\CapstoneTechX\\ag_news_cleaned', 'train.csv'), index=False)\n",
    "df_val.to_csv(os.path.join(r'C:\\Users\\nickr\\OneDrive\\Desktop\\CapstoneTechX\\ag_news_cleaned', 'test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcfdf86-2d73-4a16-83bf-292849e06163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature data directory\n",
    "\n",
    "feature_data_dir = r'C:\\Users\\nickr\\OneDrive\\Desktop\\CapstoneTechX\\features'\n",
    "os.makedirs(feature_data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d65b5e-2da9-43bf-a4ee-12528882de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization for Title\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000)  # we can play around with this. This was an arbitrary value\n",
    "train_title_features = tfidf_vectorizer.fit_transform(df_train['Title'])\n",
    "test_title_features = tfidf_vectorizer.transform(df_val['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7305e5a-8503-47ef-9a90-d9111b0a741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at one of the matrices the vectorizer produces\n",
    "\n",
    "# print(df_train['Title'][98])\n",
    "# print(train_title_features.toarray()[98]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8896a-912f-4ae6-93e7-bcc49eb426ac",
   "metadata": {},
   "source": [
    "Note, the vectorizer produces a value for a specific word on a scale of 0 to 1. The closer the number is to 1, the more unique that word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371fd926-9565-407d-b609-d86be6844fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print our features\n",
    "\n",
    "features = tfidf_vectorizer.get_feature_names_out()\n",
    "print(tfidf_vectorizer.vocabulary_, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d2d50-a90d-4f23-8dd4-fe86285d9b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm feature number\n",
    "\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67589178-9134-4c03-83f3-28437806b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF-IDF feature matrices\n",
    "\n",
    "#pd.DataFrame(train_desc_features.toarray()).to_csv(os.path.join(feature_data_dir, 'train_desc_features.csv'), index=False)\n",
    "#pd.DataFrame(test_desc_features.toarray()).to_csv(os.path.join(feature_data_dir, 'test_desc_featuress.csv'), index=False)\n",
    "#pd.DataFrame(train_title_features.toarray()).to_csv(os.path.join(feature_data_dir, 'train_title_features.csv'), index=False)\n",
    "#pd.DataFrame(test_title_features.toarray()).to_csv(os.path.join(feature_data_dir, 'test_title_featuress.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26fb14-aaca-4d59-a48a-6cf31e68d181",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f59a9-5f6f-45ec-9033-370d551893f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the counts of each index\n",
    "class_counts = df_train['Class Index'].value_counts().reset_index()\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=class_counts, x='Class Index', y='count', hue='count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d606f-03f5-479c-a1a3-0df0683ad82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of class distribution\n",
    "\n",
    "class_balance = class_counts.describe()\n",
    "print(\"Class Balance:\")\n",
    "print(class_balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3df77-f6f1-492b-931f-ee7c7c0a3998",
   "metadata": {},
   "source": [
    "Note that there is a fairly even distribution of categories in our training dataset. No further resampling techniques needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d05a8-1e8e-429e-aa32-cd2a14e7074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataframes by category\n",
    "\n",
    "index_one = df_train['Class Index'] == 1\n",
    "df_index_one = df_train[index_one]\n",
    "\n",
    "index_two = df_train['Class Index'] == 2\n",
    "df_index_two = df_train[index_two]\n",
    "\n",
    "index_three = df_train['Class Index'] == 3\n",
    "df_index_three = df_train[index_three]\n",
    "\n",
    "index_four = df_train['Class Index'] == 4\n",
    "df_index_four = df_train[index_four]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c74e1-4ad6-4d9f-92a6-fa673799697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather most common words for category World News\n",
    "\n",
    "index_list_one = ' '.join(df_index_one['Title']).split()\n",
    "word_counts_one = Counter(index_list_one)\n",
    "one_common_words = word_counts_one.most_common(30)\n",
    "print(\"\\nWorld News - Most Common Words:\")\n",
    "print(one_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf82da8-1fc8-4ac5-bafa-b571b9a24940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather most common words for category Sports News\n",
    "\n",
    "index_list_two = ' '.join(df_index_two['Title']).split()\n",
    "word_counts_two = Counter(index_list_two)\n",
    "two_common_words = word_counts_two.most_common(30)\n",
    "print(\"\\nSports News - Most Common Words:\")\n",
    "print(two_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f22728-04f9-4f71-9244-b13a7516dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather most common words for category Business News\n",
    "\n",
    "index_list_three = ' '.join(df_index_three['Title']).split()\n",
    "word_counts_three = Counter(index_list_three)\n",
    "three_common_words = word_counts_three.most_common(30)\n",
    "print(\"\\nBusiness News - Most Common Words:\")\n",
    "print(three_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca417e78-dd27-48c2-a3a0-f9e348b8ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather most common words for category Sci/Tech News\n",
    "\n",
    "index_list_four = ' '.join(df_index_four['Title']).split()\n",
    "word_counts_four = Counter(index_list_four)\n",
    "four_common_words = word_counts_four.most_common(30)\n",
    "print(\"\\nSci/Tech News - Most Common Words:\")\n",
    "print(four_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8842160-1fad-45f1-ab26-0cafea59480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize word frequency for World News\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[0] for word in one_common_words], y=[word[1] for word in one_common_words])\n",
    "plt.title('Most Common Words in World News')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee790b42-4992-44da-a93a-dbee6d57cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize word frequency for Business News\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[0] for word in two_common_words], y=[word[1] for word in two_common_words])\n",
    "plt.title('Most Common Words in Sports News')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f658c-d81c-4944-a07b-0e094e13a90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize word frequency for World News\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[0] for word in three_common_words], y=[word[1] for word in three_common_words])\n",
    "plt.title('Most Common Words in Business News')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe884d1-73f1-40be-a1a9-e6554a224ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize word frequency for World News\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[0] for word in four_common_words], y=[word[1] for word in four_common_words])\n",
    "plt.title('Most Common Words in Sci/Tech News')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c286c-7fc0-4c5f-90d7-e36a56f1d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get title lengths for each row\n",
    "df_train['Title Length'] = df_train['Title'].apply(lambda x: len(x.split()))\n",
    "print(df_train['Title Length'])\n",
    "\n",
    "# Modifying df_val for future use\n",
    "df_val['Title Length'] = df_val['Title'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8cd83-049e-4827-88bb-1abc667f7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title length analysis\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_train['Title Length'], bins=10)\n",
    "plt.title('Distribution of Title Lengths')\n",
    "plt.xlabel('Title Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b2092d-8b39-4af7-87d8-f68dae1ac8f6",
   "metadata": {},
   "source": [
    "Note that the distribution of our title lengths is right-skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596f239-a78f-42a0-a423-c89efab83248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get TF IDF values as a dataframe\n",
    "\n",
    "tfidf_df = pd.DataFrame(train_title_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648438c-becc-40db-a7d2-b8bb1a03fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform matrix to array, flatten, and removes zeros\n",
    "\n",
    "tfidf_df = train_title_features.toarray().flatten()\n",
    "tfidf_df = tfidf_df[tfidf_df != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205b3bb2-d7a4-4997-91dc-e345bd828a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of non-zero TF-IDF Scores\n",
    "\n",
    "sns.histplot(tfidf_df, bins=10, kde=True)\n",
    "plt.xlabel(\"TF-IDF Score\")\n",
    "plt.ylabel(\"Number of Words\")\n",
    "plt.title(\"Distribution of TF-IDF Scores in the Corpus\")\n",
    "plt.xticks(rotation=45)  # Optional: Rotate x-axis labels for long feature names\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30cf47b-7428-4762-a44a-33451f69311d",
   "metadata": {},
   "source": [
    "Note that our TF IDF distribution is right skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716a021-67be-41b5-8911-5c1c18825a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.outliers import Winsorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf7fae-fe84-432d-af5f-b9537b9903f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplot to find skewness\n",
    "\n",
    "sns.boxplot(df_train['Title Length'], orient='h')\n",
    "plt.xlabel(\"Title Length\")\n",
    "plt.title(\"Boxplot of Title Length\")  # Optional: Rotate x-axis labels for long feature names\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d895e-0bd3-4d4c-9c0f-44d482f59bf0",
   "metadata": {},
   "source": [
    "Confirms previous image indicating right skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a261c00-1a44-45f9-ae10-d6369a59d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorize the text length to handle outliers\n",
    "\n",
    "capper = Winsorizer(capping_method='gaussian', tail='right', fold=2)\n",
    "capper.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da52d1d-ccd4-4de8-b373-97da9f592f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where the right tail will be capped\n",
    "\n",
    "capper.right_tail_caps_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67976cd-7084-454b-a860-a473e0fe3be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform both train and validation data frames\n",
    "\n",
    "train_t = capper.transform(df_train)\n",
    "test_t = capper.transform(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66974e4b-4c06-41db-a76d-526c7ad7a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the transform worked appropriately\n",
    "\n",
    "sns.boxplot(train_t['Title Length'], orient='h')\n",
    "plt.xlabel(\"Title Length\")\n",
    "plt.title(\"Boxplot of Title Length\")  # Optional: Rotate x-axis labels for long feature names\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c64f6-9f1e-4f33-aa48-d90f45e8807e",
   "metadata": {},
   "source": [
    "Outliers are no longer present in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04017ae8-e72c-4e2b-ae4d-123d7dc15f5b",
   "metadata": {},
   "source": [
    "# Transformer Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa103aba-e341-4212-b17f-b3aee15b5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, GlobalAveragePooling1D, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed68d04e-01a6-443a-9cd9-6e45e2b49518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "def get_positional_encoding(seq_length, d_model):\n",
    "    positions = tf.range(seq_length, dtype=tf.float32)[:, tf.newaxis]\n",
    "    i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
    "    angle_rates = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    angle_rads = positions * angle_rates\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return pos_encoding\n",
    "\n",
    "\n",
    "def transformer_block(x, num_heads, d_model, dff, rate, training):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "    attn_output = Dropout(rate)(attn_output, training=training)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "    ffn_output = Dense(dff, activation='relu')(out1)\n",
    "    ffn_output = Dense(d_model)(ffn_output)\n",
    "    ffn_output = Dropout(rate)(ffn_output, training=training)\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "\n",
    "# Build the Transformer Model for Text Classification\n",
    "def build_model(max_len_input, vocab_size, num_heads=8, d_model=128, dff=512, rate=0.1):\n",
    "    # Input\n",
    "    inputs = Input(shape=(max_len_input,), name=\"input\")\n",
    "    embedding = Embedding(vocab_size, d_model, name=\"embedding\")(inputs)\n",
    "    pos_encoding = get_positional_encoding(max_len_input, d_model)\n",
    "    embedding += pos_encoding\n",
    "\n",
    "    # Transformer Encoder\n",
    "    encoder_output = embedding\n",
    "    for _ in range(4):\n",
    "        encoder_output = transformer_block(encoder_output, num_heads, d_model, dff, rate, training=True)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    pooled_output = GlobalAveragePooling1D()(encoder_output)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(pooled_output)  # Binary classification, use sigmoid activation\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])  # Binary classification, use binary_crossentropy\n",
    "\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "max_len_input = 100\n",
    "vocab_size = 10000\n",
    "\n",
    "# Create the model\n",
    "model = build_model(max_len_input, vocab_size)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b49de8-f13e-48ae-a68b-806ad1bbf227",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DistilBertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mDistilBertTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_data\u001b[39m(input_text, tokenizer, max_len_input):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Tokenize and encode input text\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text, max_length\u001b[38;5;241m=\u001b[39mmax_len_input, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DistilBertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_data(input_text, tokenizer, max_len_input):\n",
    "    # Tokenize and encode input text\n",
    "    input_ids = tokenizer.encode(input_text, max_length=max_len_input, truncation=True)\n",
    "    input_ids_padded = input_ids + [0] * (max_len_input - len(input_ids))  # Pad sequences\n",
    "    return input_ids_padded\n",
    "    \n",
    "def predict_class(input_text, tokenizer, model, max_len_input):\n",
    "    input_ids_padded = preprocess_data(input_text, tokenizer, max_len_input)\n",
    "    # Convert input to tensor\n",
    "    input_ids_tensor = tf.convert_to_tensor([input_ids_padded])\n",
    "    # Predict using the model\n",
    "    outputs = model(input_ids_tensor)\n",
    "    predicted_class = tf.argmax(outputs[0]).numpy()\n",
    "    return predicted_class\n",
    "\n",
    "predicted_class = predict_class(\"The US is in running out of oil\", tokenizer, model, max_len_input)\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f56e4b-e70f-4113-82e8-622486c21bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "import tftrainer\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DataCollatorWithPadding\n",
    "from torch import cuda\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681cb9b0-a042-4821-9439-5e198c383ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove title length from df_val\n",
    "# Only use if the Winsorizer was utilized in Feature Analysis\n",
    "\n",
    "# df_val = df_val.drop(['Title Length'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a738c-91d1-4cae-ac89-6d479f9876ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove title length from df_train\n",
    "# Only use if the Winsorizer was utilized in Feature Analysis\n",
    "\n",
    "# df_train = df_train.drop(['Title Length'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab37129-5919-4c16-876c-d7e4d024c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "\n",
    "df_train.rename(columns = {'Title':'text','Class Index':'labels'}, inplace = True)\n",
    "df_val.rename(columns = {'Title':'text','Class Index':'labels'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb7653-d249-457e-a618-68cb895ee95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable(?) GPU Acceleration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31303f70-a2f5-4d1b-a8c2-55d1d12b9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4, problem_type=\"multi_label_classification\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8af2f-dc27-4ecf-811d-3531dd112457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DistilBert\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", padding='longest', return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1197f8c-1b22-42db-9021-cca88fe1478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a3c4e-ae0c-4725-a210-3324bacf7d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print predicted class IDs\n",
    "\n",
    "predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
    "print(predicted_class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396fa15-99a4-4150-b2ef-5f4097ece5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.sum(\n",
    "    torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=4), dim=1\n",
    ").to(torch.float)\n",
    "loss = model(**inputs, labels=labels).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc88cf-8a37-493e-866a-38098facbb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df_train and df_val to a HuggingFace dataset for easier tokenization\n",
    "\n",
    "hugging_train = Dataset.from_pandas(df_train)\n",
    "hugging_val = Dataset.from_pandas(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c63fde-1e35-4611-9443-1c9cefdad952",
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe6e86-9a59-410c-92c8-8b9823ff2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96f355-ae89-416a-bda7-23ae5e6e5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the text to create input and attention data\n",
    "    \n",
    "    in -> dataset (columns = text, label)\n",
    "    out -> tokenized dataset (columns = text, label, input, attention)\n",
    "    \"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1b693-6e3d-4727-9494-c651c3b320fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = hugging_train.map(preprocess_function, batched=True)\n",
    "tokenized_val = hugging_val.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ec413-eb39-4266-9998-45e4de53ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f9c44-3e2d-4b40-87ad-847f1ec0eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883179a0-6f48-4ec5-a4f1-9635e16a5fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=1e-5,               \n",
    "    logging_dir='./logs',            \n",
    "    eval_steps=100,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_train,\n",
    "                eval_dataset=tokenized_val,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator, \n",
    "                compute_metrics='sparsecategoricalcrossentropy',\n",
    "                #compute_loss='sparsecategoricalcrossentropy',\n",
    "                #optimizers='SGD')\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf09413-a02a-47bb-80dc-3926af0c6e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc73bbb-57af-483a-9d79-32070044f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 1e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
