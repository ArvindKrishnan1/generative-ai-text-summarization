{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3760dcd7-a394-4945-8ea8-c18eddb0b189",
   "metadata": {},
   "source": [
    "# Data Importation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b9f87-2ac6-43ee-86d4-56a6659b7e9b",
   "metadata": {},
   "source": [
    "## Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1ce31-320d-420d-aa2b-b95d4f25687b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44b406-101f-4306-98d1-dc8f4dfefe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download stopwords\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newStopwords = ['b','lt','gt','n','u'] # Add stopwords\n",
    "\n",
    "for stopword in newStopwords: \n",
    "    stopwords.append(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d547f7e-4341-4177-96f6-fdc650019e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Kaggle\n",
    "# Be sure to have your Kaggle username and key\n",
    "\n",
    "dataset = \"https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset/data\"\n",
    "od.download(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ccdc3-2c5a-4901-968d-5367abafd979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset, import only 30000 rows of data\n",
    "\n",
    "df = pd.read_csv(r'your\\path\\here\\ag_news.csv',nrows=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe70ee-1098-408f-9f0f-e71f6b6f8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm importation\n",
    "\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac867b1-d70d-4303-a620-7deae2ceeb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of dataframe\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463838c-004e-49a5-aa71-c5ce57c5001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Class Index to categorical variable\n",
    "\n",
    "df['Class Index']=df['Class Index'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459fc0a4-26e8-491e-9878-4506ac15c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find null values and datatypes\n",
    "\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7eccb9-a304-4709-b4ff-820c6c30fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76063e5d-5d53-4a7c-a20b-b84b0404ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data set html, special, and non-textual characters\n",
    "\n",
    "def cleaning_text(text):\n",
    "    # Remove HTML tags\n",
    "    cleaning_text = re.sub('<.*?>', '', text)\n",
    "    # Remove special characters and non-textual \n",
    "    cleaning_text = re.sub(r'([^a-zA-Z\\s]|\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', ' ', cleaning_text) # checks plain text for given characters\n",
    "    return cleaning_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445aca4f-9b91-4065-8651-8de03e6d3683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the function worked\n",
    "\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f55122a-0009-430b-abcf-70448519e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to remove stop words\n",
    "\n",
    "stop_words = set(stopwords)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization and lowercasing\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Stop word removal\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e72cc-5cda-49de-9c81-cf3eca2103ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing to text in Title\n",
    "\n",
    "df['Title'] = df['Title'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb17a3-a8a9-48a3-a3d7-4f57c2b568e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the function worked\n",
    "\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728734c0-2ef3-4aa4-b412-33b55d57cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to CSV for ease of use in future\n",
    "\n",
    "cleaned_data_file = r'your\\path\\here\\cleaned_ag_news.csv'\n",
    "df.to_csv(cleaned_data_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af771aa-5ebd-4041-9260-f3ec3d290e1f",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78767cde-1076-4d6b-b979-2d85bf33e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation data \n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96903ec6-66e6-4bb3-af43-8f6a31da57ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv file for train and test data\n",
    "\n",
    "df_train.to_csv(os.path.join(r'your\\path\\here', 'train.csv'), index=False)\n",
    "df_test.to_csv(os.path.join(r'your\\path\\here', 'test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b5925a-28a3-44f7-83a0-995962e68cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature data directory\n",
    "\n",
    "feature_data_dir = r'your\\path\\here\\features'\n",
    "os.makedirs(feature_data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a67e67-3925-4bc1-a6e0-bcc9c462041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization for Title\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000)  # we can play around with this. This was an arbitrary value\n",
    "train_title_features = tfidf_vectorizer.fit_transform(df_train['Title'])\n",
    "test_title_features = tfidf_vectorizer.transform(df_test['Title'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
