{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1224ff1-aeff-4e29-b202-481832e55ab0",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e6f12-da61-4f0b-ab64-3526080a77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import s3fs\n",
    "import fs_s3fs\n",
    "import fsspec\n",
    "import json\n",
    "from llama_index.core import TreeIndex, SimpleDirectoryReader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import torch\n",
    "import transformers\n",
    "import mlflow\n",
    "import hyperopt as hp\n",
    "import sphinx\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbad013-ca4e-4666-9764-f040416cb0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762117e8-eeaf-45ea-98d6-4bbfca6447d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8854c533-9311-4cc4-85df-42be1ef2413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup python environment\n",
    "\n",
    "# !python -m venv C:\\Users\\nickr\\OneDrive\\Documents\\GitHub\\generative-ai-text-summarization\\config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5b57f-b8de-46d5-a7fd-f5de9f7b5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "df_train = pd.read_csv(r\"C:\\Users\\nickr\\OneDrive\\Desktop\\CapstoneTechX\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925624a-4c1d-4927-808f-5975d06b0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head() # Confirm importation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53803b54-f203-406b-9cfc-3931fa943371",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737225c-a1f0-4109-abde-5c2d75506d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find null values and datatypes\n",
    "\n",
    "df_train.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be72a3e-3d19-4901-abdb-7fa1006122fb",
   "metadata": {},
   "source": [
    "There are no null values in the df_train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6816ed8-6b16-4f5a-8e8f-86c6745b22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "\n",
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8028175-62cd-4820-a191-1e6ae3652326",
   "metadata": {},
   "source": [
    "There are no duplicate values in the df_train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8c488-1f0f-4f07-bf1a-93b1e604194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data set html, special, and non-textual characters\n",
    "\n",
    "def cleaning_text(text):\n",
    "    # Remove HTML tags\n",
    "    cleaning_text = re.sub('<.*?>', '', text)\n",
    "    # Remove special characters and non-textual \n",
    "    cleaning_text = re.sub(r'([^a-zA-Z\\s]|\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', ' ', cleaning_text) # checks plain text for given characters\n",
    "    return cleaning_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479d5c2-b4e2-4c30-9faf-3051b423d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply text cleaning to text in both Description and Title\n",
    "\n",
    "df_train['Description'] = df_train['Description'].apply(cleaning_text)\n",
    "df_train['Title'] = df_train['Title'].apply(cleaning_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a1d06-2735-4db7-86ec-acc992362765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0223382e-0fd0-4e2a-a2ce-58bf02a2b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to remove stop words\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization and lowercasing\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Stop word removal\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e8790-b96c-4391-96c5-1ceb2209ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing to text in both Description and Title\n",
    "\n",
    "df_train['Description'] = df_train['Description'].apply(preprocess_text)\n",
    "df_train['Title'] = df_train['Title'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85872e09-1b0f-4991-9337-10ffec72a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e870c4d-abfd-4af9-b3e3-200b57aea54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to CSV for ease of use in future\n",
    "\n",
    "cleaned_data_file = r'C:\\Users\\nickr\\OneDrive\\Documents\\GitHub\\generative-ai-text-summarization\\data\\cleaned_ag_news.csv'\n",
    "df_train.to_csv(cleaned_data_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd8cdb-add6-4426-84db-803400bbc1f3",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d4286e-9003-4783-b52f-b2a3754fbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation data \n",
    "\n",
    "df_train, df_test = train_test_split(df_train, test_size=.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183cea1-30ce-46b1-9a88-60e5a0dcb957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
